import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

print("ğŸ§  å¼€å§‹äºŒåˆ†ç±»æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼šé¢„æµ‹æ˜¯å¦ä¸ºæ°§åŒ–è¿˜åŸé…¶")

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('HDOs_with_features.csv')
print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")

# 2. åˆ›å»ºäºŒåˆ†ç±»ç›®æ ‡å˜é‡ï¼šæ˜¯å¦ä¸ºæ°§åŒ–è¿˜åŸé…¶ï¼ˆEC 1ç±»ï¼‰
def is_oxidoreductase(ec_str):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ°§åŒ–è¿˜åŸé…¶ï¼ˆEC 1ç±»ï¼‰"""
    if pd.isna(ec_str):
        return 0  # æ²¡æœ‰ECç¼–å·çš„è§†ä¸ºéæ°§åŒ–è¿˜åŸé…¶
    ec_str = str(ec_str)
    return 1 if ec_str.startswith('1.') else 0

df['is_oxidoreductase'] = df['EC number'].apply(is_oxidoreductase)
print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
print(f"  æ°§åŒ–è¿˜åŸé…¶ (EC 1ç±»): {df['is_oxidoreductase'].sum()} æ¡")
print(f"  éæ°§åŒ–è¿˜åŸé…¶: {len(df) - df['is_oxidoreductase'].sum()} æ¡")
print(f"  æ¯”ä¾‹: {df['is_oxidoreductase'].mean():.2%}")

# 3. å‡†å¤‡ç‰¹å¾
exclude_cols = ['Entry', 'Entry Name', 'Protein names', 'Gene Names', 
                'Organism', 'Sequence', 'EC number', 'Function [CC]', 
                'Cofactor', 'Keywords', 'Reviewed', 'is_oxidoreductase']

feature_columns = [col for col in df.columns if col not in exclude_cols]
X = df[feature_columns].fillna(df[feature_columns].median())
y = df['is_oxidoreductase']

print(f"\nğŸ”§ ç‰¹å¾å·¥ç¨‹:")
print(f"  ç‰¹å¾æ•°é‡: {X.shape[1]}")
print(f"  æ ·æœ¬æ•°é‡: {X.shape[0]}")

# 4. å¤„ç†é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼ˆç§»é™¤å†—ä½™ç‰¹å¾ï¼‰
# ç§»é™¤ 'Length'ï¼ˆä¿ç•™ 'length' å’Œ 'molecular_weight' ä¸­çš„ä¸€ä¸ªï¼‰
if 'Length' in feature_columns:
    feature_columns.remove('Length')
if 'length' in feature_columns:
    feature_columns.remove('length')  # ä¿ç•™ 'molecular_weight'
X = df[feature_columns].fillna(df[feature_columns].median())

# 5. æ•°æ®æ ‡å‡†åŒ–å’Œåˆ’åˆ†
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 6. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
print("\nğŸŒ² è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 7. æ¨¡å‹è¯„ä¼°
y_pred = rf_model.predict(X_test)
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]

accuracy = (y_pred == y_test).mean()
roc_auc = roc_auc_score(y_test, y_pred_proba)

print("\nğŸ¯ æ¨¡å‹æ€§èƒ½:")
print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
print(f"  ROC AUC: {roc_auc:.3f}")
print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=['éæ°§åŒ–è¿˜åŸé…¶', 'æ°§åŒ–è¿˜åŸé…¶']))

# 8. ç‰¹å¾é‡è¦æ€§åˆ†æ
feature_importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nğŸ” æœ€é‡è¦çš„10ä¸ªç‰¹å¾:")
for idx, row in feature_importance.head(10).iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# 9. ä¿å­˜ç»“æœ
joblib.dump(rf_model, 'rf_binary_classifier.pkl')
joblib.dump(scaler, 'binary_scaler.pkl')
feature_importance.to_csv('binary_feature_importance.csv', index=False)

print("\nğŸ’¾ å·²ä¿å­˜:")
print("  rf_binary_classifier.pkl - éšæœºæ£®æ—æ¨¡å‹")
print("  binary_scaler.pkl - æ ‡å‡†åŒ–å™¨")
print("  binary_feature_importance.csv - ç‰¹å¾é‡è¦æ€§")

# 10. å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# ç‰¹å¾é‡è¦æ€§
top10 = feature_importance.head(10)
axes[0].barh(range(10), top10['importance'][::-1])
axes[0].set_yticks(range(10))
axes[0].set_yticklabels(top10['feature'][::-1])
axes[0].set_xlabel('ç‰¹å¾é‡è¦æ€§')
axes[0].set_title('é¢„æµ‹æ°§åŒ–è¿˜åŸé…¶çš„å…³é”®ç‰¹å¾')

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_xlabel('é¢„æµ‹ç±»åˆ«')
axes[1].set_ylabel('çœŸå®ç±»åˆ«')
axes[1].set_xticklabels(['éæ°§åŒ–è¿˜åŸé…¶', 'æ°§åŒ–è¿˜åŸé…¶'])
axes[1].set_yticklabels(['éæ°§åŒ–è¿˜åŸé…¶', 'æ°§åŒ–è¿˜åŸé…¶'])
axes[1].set_title('æ··æ·†çŸ©é˜µ')

plt.tight_layout()
plt.savefig('binary_classification_results.png', dpi=300, bbox_inches='tight')
print("\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: binary_classification_results.png")

print("\nâœ… äºŒåˆ†ç±»å»ºæ¨¡å®Œæˆï¼")
print("\nğŸ“ å¤§ä½œä¸šæŠ¥å‘Šè¦ç‚¹:")
print("1. ç ”ç©¶é—®é¢˜: ä»è›‹ç™½è´¨åºåˆ—ç‰¹å¾é¢„æµ‹å…¶æ˜¯å¦ä¸ºæ°§åŒ–è¿˜åŸé…¶")
print("2. æ•°æ®æ¥æº: 368æ¡åŒé“é‡‘å±é…¶ï¼Œ73.6%ä¸ºæ°§åŒ–è¿˜åŸé…¶")
print(f"3. æ¨¡å‹æ€§èƒ½: éšæœºæ£®æ—å‡†ç¡®ç‡{accuracy:.3f}, ROC AUC{roc_auc:.3f}")
print("4. å…³é”®å‘ç°: æ­ç¤ºäº†åºåˆ—ç‰¹å¾ä¸æ°§åŒ–è¿˜åŸé…¶åŠŸèƒ½çš„å…³ç³»")
print("5. åº”ç”¨ä»·å€¼: å¯ç”¨äºå¿«é€Ÿæ³¨é‡ŠæœªçŸ¥è›‹ç™½çš„åŠŸèƒ½ç±»åˆ«")